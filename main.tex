\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Enhancing Pedagogical Feedback Quality of Large Language Models via Learning Analytics in a Kanban-Based Platform for Student Self-Regulated Learning Support}

\author{
\IEEEauthorblockN{1\textsuperscript{st} Qornain Aji}
\IEEEauthorblockA{\textit{Department of Electrical and}\\
\textit{Information Engineering} \\
\textit{Universitas Gadjah Mada}\\
Yogyakarta, Indonesia\\
qornain.aji@mail.ugm.ac.id}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Indriana Hidayah}
\IEEEauthorblockA{\textit{Department of Electrical and}\\
\textit{Information Engineering} \\
\textit{Universitas Gadjah Mada}\\
Yogyakarta, Indonesia\\
indriana.h@ugm.ac.id}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Syukron Abu Ishaq A.}
\IEEEauthorblockA{\textit{Department of Electrical and}\\
\textit{Information Engineering} \\
\textit{Universitas Gadjah Mada}\\
Yogyakarta, Indonesia\\
syukron.abu@ugm.ac.id}
}

\maketitle

\begin{abstract}
% This document is a model and instructions for \LaTeX.
% This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
% or Math in Paper Title or Abstract.
This study aims to enhance the quality of pedagogical 
feedback generated by Large Language Models (LLMs) to 
support student Self-Regulated Learning (SRL), addressing 
the scalability crisis of human expert feedback. The 
enhancement focuses on three core functions such as 
feedback, motivation, and appreciation. A comparative 
experimental method was employed without fine-tuning, 
evaluating various Context Engineering schemes enriched 
with Learning Analytics (LA) on two models: Llama 3.1 8B 
and Llama 3.3 70B. Evaluation was quantitative (BERTScore, 
BARTScore, LLM-as-a-Judge) and qualitatively validated by 
educational psychology experts.

Results indicate that the effectiveness of the enhancement 
significantly depends on the model size. The smaller model 
(8B) achieved the highest performance increase with complex 
context (ReAct + LA), suggesting a reliance on rich, structured 
guidance to compensate for its internal limitations. 
Conversely, the larger model (70B), despite having a superior 
Baseline performance, showed degraded performance when given 
overly complex context. The best-performing model (Llama 8B 
with ReAct + LA) was validated by human experts as "Usable 
with minor revision". The study concludes that LA-enriched 
context engineering is an effective, practical, and computationally 
efficient strategy for improving pedagogical feedback, particularly 
for smaller LLMs.

% Keywords: Large Language Model, Self-Regulated Learning, Learning 
% Analytics, Context Engineering, Pedagogical Feedback.
\end{abstract}

\begin{IEEEkeywords}
Large Language Model, Self-Regulated Learning, Learning Analytics, 
Context Engineering, Pedagogical Feedback
\end{IEEEkeywords}

\section{Introduction}
% This document is a model and instructions for \LaTeX.
% Please observe the conference page limits. 
The current landscape of higher education demands that s
tudents possess the ability to learn independently. However, 
many are unprepared to face these demands. This creates a 
serious gap in \textit{Self-Regulated Learning} (SRL) skills, 
which can hinder learning success and disrupt equitable access 
to education \cite{liu2025srl, xiao2019relationships}. This challenge 
is exacerbated by the digital technologies that characterize modern 
learning. While offering flexibility, these technologies also present 
significant distractions and require greater self-discipline 
\cite{chitra2022srl}. Institutional failure to provide adequate 
support for SRL development is not merely an academic issue; it is a 
social issue that impacts students' long-term career prospects and the 
overall effectiveness of the education system \cite{liu2025srl}. 
% ------------------------------------------------

To address the SRL gap, Zimmerman proposed a framework encompassing 
the phases of planning (\textit{forethought}), execution 
(\textit{performance}), and self-reflection (\textit{self-reflection}), 
wherein SRL is a combination of cognitive, motivational, and 
behavioral strategies \cite{liu2025srl, akdeniz2022srl}. 
Individuals with effective self-regulation capabilities are 
formed through goal setting, strategy planning, active 
self-monitoring, and thoughtful self-evaluation. A substantial 
body of research confirms a strong relationship between these 
skills and academic achievement \cite{liu2025srl, xiao2019relationships}. 
However, their development is often hindered by personal, 
contextual, and social factors, ranging from poor time 
management and digital distractions to low metacognitive 
awareness. Metacognition itself refers to the knowledge, 
awareness, and regulation of one's own thinking, playing a 
crucial role in building students' awareness of their 
learning processes \cite{zimmerman2009selfregulation}. 
These barriers necessitate structured interventions to guide 
students through this complex process \cite{chitra2022srl, 
10.3389/feduc.2024.1418297}.

The most effective traditional method for developing SRL 
skills is personalized feedback from human experts. 
High-quality feedback that focuses not only on the quantity 
of completed tasks but also on the student's learning process 
and self-regulation serves as a powerful catalyst for 
improvement [15, 16]\cite{10.3389/fpsyg.2022.1027266, Bock14032024}. 
However, this "gold standard" faces a scalability crisis; increasing 
student numbers and diverse support needs make the personalized 
guidance model difficult to implement on a large scale. This is 
reinforced by studies showing that budgetary pressures drive larger 
class sizes, thereby severely limiting personal interaction between 
lecturers and students \cite{schaffer2017automating, pardo2019using}. 
This creates an urgent need for innovative technology-based solutions 
that can complement human expertise.

One such technology is the use of \textit{Large Language Models} 
(LLMs). LLMs can be employed as virtual tutors capable of providing 
fast and consistent feedback. LLMs have been utilized in various 
educational research contexts with diverse breakthroughs. Recent 
findings show that when LLMs are used for assessment and feedback, 
\textit{artificial intelligence} (AI) tends to be generous in 
grading, resulting in scores higher than those given by human 
experts. Meanwhile, peer and lecturer assessments tend to be 
lower and more aligned with student performance. AI feedback is 
often structured but still requires human expert oversight to 
remain pedagogically relevant, leading to the recommendation of 
a hybrid approach (combining AI and human experts) 
\cite{Usher2025GenAIAssessments}. A hybrid approach is 
recommended as it can combine the speed and scalability of AI 
with the contextual mastery, empathetic understanding, and 
pedagogical nuances of human assessment 
\cite{Usher2025GenAIAssessments}. This recommendation underscores 
that LLM responses still require expert supervision to be relevant. 
On the other hand, efforts to standardize AI-tutor evaluation 
indicate that although current models like GPT-4 are strong in 
answering questions, they tend to provide answers to students 
too quickly and lack process guidance. This fact suggests that 
LLMs are not yet ideal as tutors and cannot yet replace human 
experts in assessing pedagogical quality 
\cite{Maurya2025UnifyingAITutorEvaluation}.

Therefore, this thesis focuses on improving the performance 
quality of LLM responses as feedback providers to approach 
the quality of human experts, both quantitatively and 
qualitatively. This improvement is executed through an 
evaluation framework that combines human expert references as 
\textit{ground truth} with quality assessments evaluating 
relevant pedagogical dimensions \cite{Usher2025GenAIAssessments, 
Maurya2025UnifyingAITutorEvaluation}. 
% ------------------------------------------------- 
To support this measurement process, this thesis utilizes 
structured learning process traces. Student learning data, 
presented in the form of a Kanban board, is used solely as 
an operational tool to organize and extract process data 
(card movement, checklists, completion time) that can be 
mapped to SRL phases (planning-monitoring-reflecting) 
\cite{strickroth2022kanban}. The use of the Kanban board 
serves only as a data enrichment instrument to ensure the 
evaluation and improvement of LLM feedback quality proceeds 
more purposefully. In this research, as explained in the 
subsequent chapter, the focus of novelty lies in the series 
of evaluations and refinements of the LLM \textit{prompts}. 
This research aims to improve three main pedagogical 
functions—feedback, motivational support, and appreciative 
support—with the practical goal of matching human expert 
quality on quantitative metrics while simultaneously meeting 
qualitative quality standards \cite{10.3389/fpsyg.2022.1027266, 
Bock14032024, Usher2025GenAIAssessments, 
Maurya2025UnifyingAITutorEvaluation}. By comparing LLM 
outputs with expert specialists, this study assesses the 
improvement in LLM performance for consideration as a 
massive yet personalized complementary pedagogical agent 
for students.

\section{Related Works}
% \subsection{Improving LLM Feedback with \textit{Direct Preference Optimization} (DPO)}
Woodrow, Koyejo, and Piech \cite{woodrow2025dpo_feedback} address 
the \textit{Feedback Alignment Challenge}, namely the tendency 
of LLM-generated feedback to be generic and misaligned with 
course-specific rigor, terminology, and instructor preferences. 
They propose \textit{Direct Preference Optimization} (DPO) with 
teachers in the loop, where TAs select or edit preferred outputs 
from model pairs during grading, producing preference data that 
enables fine-tuning without a reward model as in RLHF. Their 
three-stage pipeline (preference collection, DPO training, inference) 
forms a self-improving system that increasingly aligns with 
course expectations across assignments. Using Llama~3.1~8B 
fine-tuned with HuggingFace DPOTrainer, they conducted controlled 
blind evaluations with experts and real deployments in large 
university courses, supported by automated assessment frameworks 
(e.g., Scarlatos et al.~\cite{scarlatos2024validity}). Results 
show that DPO outperformed GPT-4o in preference (56.8\% vs.\ 40.2\%), 
insightfulness, correctness, tone positivity, and consistency, 
though GPT-4o remained stronger in conciseness and suggestions. 
Deployment demonstrated feasibility at scale and potential for 
fairness monitoring, positioning DPO as a practical strategy for 
course-aligned AI feedback generation.

% \subsection{Improving LLM \textit{Feedback} with \textit{Prompt Engineering}}
Jacobsen and Weber \cite{jacobsen2025promises} examine how prompt 
quality shapes LLM feedback effectiveness in higher education and 
compare LLM-generated feedback with novice and expert human 
feedback. They develop a theory-based prompt manual covering 
role/context specification, mission clarity, instructional 
logic, and domain specificity, then craft low-, medium-, and 
high-quality prompts to systematically test prompt effects using 
ChatGPT-4. Across a geometry learning task involving typical 
student misconceptions, 60 LLM outputs were coded with 
pedagogical criteria (e.g., concreteness, activation, empathy, 
specificity) and compared with feedback written by preservice 
teachers and experts using the same high-quality prompt. Results 
show that only high-quality prompts reliably produce strong 
LLM feedback; LLMs outperform novices in most categories and 
even exceed experts in explanation, guiding questions, and 
specificity, while affective dimensions remain mixed. 
Efficiency gains were substantial, as ChatGPT-4 produced 
dozens of feedback instances in the time an expert crafted 
one. The authors highlight the necessity of structured prompt 
design training for educators and note limitations such as 
hallucination risks and the need to test student perceptions 
and transferability across contexts.

% \subsection{\textit{Reinforcement Learning from Human Feedback} (RLHF) for Response Quality Improvement}
Ouyang et al.\ \cite{ouyang2022traininglanguagemodelsfollow} 
introduce an RLHF pipeline to align language models with user 
intent, addressing limitations of large pretrained models that 
may generate incorrect, unhelpful, or unsafe outputs despite 
high linguistic proficiency. Their three-stage approach 
includes supervised fine-tuning on human demonstrations, 
reward model (RM) training from human comparisons, and policy 
optimization with PPO using RM predictions as rewards. Drawing 
on prompts from labelers and real API usage, they train reward 
models (initialized from GPT-3 6B) and RLHF policies with careful 
stability controls (KL penalties, learning rate warmup, single-epoch 
RM training to avoid overfitting). The resulting InstructGPT models 
show substantial alignment gains: the 1.3B RLHF model is 
preferred over the much larger GPT-3 175B baseline, while 
exhibiting improved truthfulness, reduced toxicity, and minimal 
regression on NLP benchmarks. The study demonstrates that human 
feedback alignment can outperform mere parameter scaling, while 
acknowledging remaining errors and offering a blueprint for 
instruction-following models widely adopted today.

% \subsection{Self-Regulation in LLM-Based Programming Education with \textit{Learning Analytics}}
Li and Ma \cite{li2025designaipoweredtoolselfregulation} propose 
CodeRunner Agent, an AI-powered system integrated into Moodle to 
support self-regulated learning (SRL) in programming education, 
addressing limitations of external LLM tools that lack course context 
and produce feedback disconnected from curricula or student 
behavioral data. The system combines a lecture viewer, CodeRunner 
execution/grading, and xAPI-based learning analytics to capture 
rich process data. Its pedagogical foundation is the PPESS model 
(Planning, Program creation, Error correction, Self-monitoring, 
Self-reflection), allowing students to request AI help targeted 
to their current SRL phase. Two context engines drive feedback 
relevance: LACE (Learning Analytics Context Engine) summarizes 
engagement and performance patterns from logs, while KCE (Knowledge 
Context Engine) manages curated course materials, concepts, 
solution structures, and typical errors. These engines inject 
SRL and knowledge-context cues into prompts to ensure feedback 
aligns with curriculum goals while avoiding spoon-feeding. The 
paper presents a design and planned evaluations—including 
behavioral analytics and qualitative assessments—to validate 
impact on SRL, code performance, and pedagogical safety, 
highlighting the feasibility of an end-to-end, context-aware, 
LMS-native AI support system.

% \subsection{\textit{Real-time} Dashboard for Kanban-Based Classroom Orchestration}
Strickroth, Kreidenweis, and Götzfried propose a real-time, 
networked extension of AgileBoard4Teaching to address 
orchestration challenges in collaborative, task-based 
classrooms where teachers struggle to monitor diverse group 
progress simultaneously. Their client-server system includes
 an authoring mode for task setup, a Kanban interface for 
 students, and a teacher dashboard that displays per-group 
 progress summaries, pending reviews, hand-raise signals, 
 timers, messaging, and live synchronization via WebSockets, 
 with storage handled through Java Servlets and MariaDB. Two 
 evaluations were conducted: a field study with 8th-grade 
 students using a within-subjects comparison of offline 
 versus networked versions, and a simulation study with 
 experienced teachers. Findings show strong preference for 
 the networked system, improved workflow fluency, high 
 usability scores (student SUS 84, teacher SUS 92), and high 
 engagement observed in interaction logs; the simulation 
 study also yielded positive reception despite some bugs. 
 Teachers valued real-time monitoring and ease of scaling 
 boards, suggesting refinements for large-class readability 
 and analytics features. Overall, the study demonstrates the 
 feasibility and pedagogical value of real-time dashboards 
 for classroom orchestration.

In light of the comparative review, this study adopts a 
pragmatic hybrid approach that prioritizes resource-efficient 
context alignment, namely high-quality prompt engineering 
and the injection of learning-analytics signals from Kanban 
process data, while maintaining strong pedagogical oversight. 
Instead of relying on costly full-model retraining, the method 
enriches teacher-authored prompts with contextual features such 
as task states, time-on-task, and review requests to make LLM 
feedback more course and SRL aware. Preference-based fine-tuning 
approaches (e.g., DPO or RLHF) are considered only as long-term 
options when sufficient preference labels and computational 
resources are available. Evaluation will combine quantitative 
comparisons of LLM outputs against expert-based rubrics and 
automated metrics with qualitative expert judgment to ensure 
pedagogical soundness. The accompanying real-time dashboard 
further enables continuous monitoring and iterative refinement 
of prompts and contextual cues. Altogether, this design aims 
to deliver a scalable, low-cost pipeline that leverages 
prompt/context engineering and learning-analytics signals to 
generate specific, actionable, and pedagogically aligned 
feedback while keeping instructors firmly in the loop.

% \subsection{Maintaining the Integrity of the Specifications}

% The IEEEtran class file is used to format your paper and style the text. All margins, 
% column widths, line spaces, and text fonts are prescribed; please do not 
% alter them. You may note peculiarities. For example, the head margin
% measures proportionately more than is customary. This measurement 
% and others are deliberate, using specifications that anticipate your paper 
% as one part of the entire proceedings, and not as an independent document. 
% Please do not revise any of the current designations.

\section{Methodology}
The research was conducted through several stages. The research
starting with a comprehensive literature review to understand
the current methods for improving LLM performance in producing
higher feedback quality. This review covered techniques such as
Reinforcement Learning from Human Feedback (RLHF), Direct Preference
Optimization (DPO), and Context Engineering, as well as the use of
Learning Analytics (LA) to enrich context for LLMs. The literature
review also examined the pedagogical aspects of feedback in
supporting Self-Regulated Learning (SRL) and the use of Kanban-based
platforms for organizing student learning processes.
After identifying the research gap, the next step involved selecting
various LLM models and prompting techniques for evaluation.

% Before you begin to format your paper, first write and save the content as a 
% separate text file. Complete all content and organizational editing before 
% formatting. Please note sections \ref{AA}--\ref{SCM} below for more information on 
% proofreading, spelling and grammar.

% Keep your text and graphic files separate until after the text has been 
% formatted and styled. Do not number text heads---{\LaTeX} will do that 
% for you.

\subsection{Tools}\label{AA}
The tools used in this final project consist of both 
hardware and software as supporting resources. The 
hardware employed is personally owned. The function 
of these hardware and software tools is to facilitate 
code development, chatbot implementation, quantitative 
and qualitative evaluations, as well as dataset collection. 
The following is the list of tools used in this research 
project.

\begin{enumerate}
    \item \textbf{Hardware:}
        \begin{itemize}
            \item MacBook Pro 14 inch: M1 Pro 10-core processor @ 3.2 GHz
            \item Internal GPU: M1 Pro 16-core GPU
            \item Neural Engine: 32-core
            \item RAM: 16 GB Unified Memory (200 GB/s memory bandwidth)
            \item Storage: 1 TB internal Solid State Drive (macOS 26 Tahoe)
        \end{itemize}
        \item \textbf{Software:}
            \begin{itemize}
                \item Visual Studio Code version 1.105.1: integrated development environment (IDE) for writing and managing code.
                \item Google Docs: tool for drafting questionnaires, performing revisions, and documenting discussion results.
                \item Google Sheets: used to visualize and manage Kanban data to share with experts for completion and assessment, facilitating collaboration and metric collection.
                \item Anaconda: Python package and virtual environment manager used during development.
            \end{itemize}
\end{enumerate}

\subsection{Units}
\begin{itemize}
\item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
\item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
\item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
\item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
\end{itemize}

\subsection{Equations}
Number equations consecutively. To make your 
equations more compact, you may use the solidus (~/~), the exp function, or 
appropriate exponents. Italicize Roman symbols for quantities and variables, 
but not Greek symbols. Use a long dash rather than a hyphen for a minus 
sign. Punctuate equations with commas or periods when they are part of a 
sentence, as in:
\begin{equation}
a+b=\gamma\label{eq}
\end{equation}

Be sure that the 
symbols in your equation have been defined before or immediately following 
the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at 
the beginning of a sentence: ``Equation \eqref{eq} is . . .''

\subsection{\LaTeX-Specific Advice}

Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
of ``hard'' references (e.g., \verb|(1)|). That will make it possible
to combine sections, add equations, or change the order of figures or
citations without having to go through the file line by line.

Please don't use the \verb|{eqnarray}| equation environment. Use
\verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
environment leaves unsightly spaces around relation symbols.

Please note that the \verb|{subequations}| environment in {\LaTeX}
will increment the main equation counter even when there are no
equation numbers displayed. If you forget that, you might write an
article in which the equation numbers skip from (17) to (20), causing
the copy editors to wonder if you've discovered a new method of
counting.

{\BibTeX} does not work by magic. It doesn't get the bibliographic
data from thin air but from .bib files. If you use {\BibTeX} to produce a
bibliography you must send the .bib files. 

{\LaTeX} can't read your mind. If you assign the same label to a
subsubsection and a table, you might find that Table I has been cross
referenced as Table IV-B3. 

{\LaTeX} does not have precognitive abilities. If you put a
\verb|\label| command before the command that updates the counter it's
supposed to be using, the label will pick up the last counter to be
cross referenced instead. In particular, a \verb|\label| command
should not go before the caption of a figure or a table.

Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
will not stop equation numbers inside \verb|{array}| (there won't be
any anyway) and it might stop a wanted equation number in the
surrounding equation.

\subsection{Some Common Mistakes}\label{SCM}
\begin{itemize}
\item The word ``data'' is plural, not singular.
\item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
\item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
\item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
\item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
\item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
\item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
\item Do not confuse ``imply'' and ``infer''.
\item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
\item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
\item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
\end{itemize}
An excellent style manual for science writers is \cite{b7}.

\subsection{Authors and Affiliations}
\textbf{The class file is designed for, but not limited to, six authors.} A 
minimum of one author is required for all conference articles. Author names 
should be listed starting from left to right and then moving down to the 
next line. This is the author sequence that will be used in future citations 
and by indexing services. Names should not be listed in columns nor group by 
affiliation. Please keep your affiliations as succinct as possible (for 
example, do not differentiate among departments of the same organization).

\subsection{Identify the Headings}
Headings, or heads, are organizational devices that guide the reader through 
your paper. There are two types: component heads and text heads.

Component heads identify the different components of your paper and are not 
topically subordinate to each other. Examples include Acknowledgments and 
References and, for these, the correct style to use is ``Heading 5''. Use 
``figure caption'' for your Figure captions, and ``table head'' for your 
table title. Run-in heads, such as ``Abstract'', will require you to apply a 
style (in this case, italic) in addition to the style provided by the drop 
down menu to differentiate the head from the text.

Text heads organize the topics on a relational, hierarchical basis. For 
example, the paper title is the primary text head because all subsequent 
material relates and elaborates on this one topic. If there are two or more 
sub-topics, the next level head (uppercase Roman numerals) should be used 
and, conversely, if there are not at least two sub-topics, then no subheads 
should be introduced.

\subsection{Figures and Tables}
\paragraph{Positioning Figures and Tables} Place figures and tables at the top and 
bottom of columns. Avoid placing them in the middle of columns. Large 
figures and tables may span across both columns. Figure captions should be 
below the figures; table heads should appear above the tables. Insert 
figures and tables after they are cited in the text. Use the abbreviation 
``Fig.~\ref{fig}'', even at the beginning of a sentence.

\begin{table}[htbp]
\caption{Table Type Styles}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Table}&\multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
\cline{2-4} 
\textbf{Head} & \textbf{\textit{Table column subhead}}& \textbf{\textit{Subhead}}& \textbf{\textit{Subhead}} \\
\hline
copy& More table copy$^{\mathrm{a}}$& &  \\
\hline
\multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
\end{tabular}
\label{tab1}
\end{center}
\end{table}

\begin{figure}[htbp]
\centerline{\includegraphics{fig1.png}}
\caption{Example of a figure caption.}
\label{fig}
\end{figure}

Figure Labels: Use 8 point Times New Roman for Figure labels. Use words 
rather than symbols or abbreviations when writing Figure axis labels to 
avoid confusing the reader. As an example, write the quantity 
``Magnetization'', or ``Magnetization, M'', not just ``M''. If including 
units in the label, present them within parentheses. Do not label axes only 
with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization 
\{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of 
quantities and units. For example, write ``Temperature (K)'', not 
``Temperature/K''.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in America is without 
an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B. 
G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor 
acknowledgments in the unnumbered footnote on the first page.

\section*{References}

Please cite all your references \cite{2023Bhawal-SST},\cite{2023Chen-Phase-Shift-DAB}. References are stored in a bibtex file "references.bib". You can use Mendeley or Jabref for your reference manager. 

%Please number citations consecutively within brackets \cite{b1}. The 
%sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
%number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
%the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''
%
%Number footnotes separately in superscripts. Place the actual footnote at 
%the bottom of the column in which it was cited. Do not put footnotes in the 
%abstract or reference list. Use letters for table footnotes.
%
%Unless there are six authors or more give all authors' names; do not use 
%``et al.''. Papers that have not been published, even if they have been 
%submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
%that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
%Capitalize only the first word in a paper title, except for proper nouns and 
%element symbols.
%
%For papers published in translation journals, please give the English 
%citation first, followed by the original foreign-language citation \cite{b6}.



\bibliography{references}{}
\bibliographystyle{IEEEtran}

%\begin{thebibliography}{00}
%\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
%\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
%\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
%\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
%\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
%\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
%\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
%\end{thebibliography}



\end{document}
